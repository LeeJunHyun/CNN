{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import os\n",
    "from scipy.misc import toimage\n",
    "from scipy import signal\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "# imshow(image)\n",
    "# plt.show()\n",
    "# toimage(image).show()\n",
    "\n",
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_config = tf.ConfigProto(device_count={'GPU':1})  # only use GPU no.1 change this!\n",
    "gpu_config.gpu_options.allow_growth = True # only use required resource(memory)\n",
    "gpu_config.gpu_options.per_process_gpu_memory_fraction = 0.6 # restrict to 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def search(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    full_filenames = []\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "        full_filenames.append(full_filename)\n",
    "        #print (full_filename)\n",
    "    return full_filenames\n",
    "\"\"\"\n",
    "def Image_search(Image_dir,num_image):\n",
    "    images=[]\n",
    "    for i in range(num_image):\n",
    "        image_raw = imread(Image_dir[i])\n",
    "        image = imresize(image_raw,(300,300))\n",
    "        images.append(image)\n",
    "    return images\n",
    "\"\"\"\n",
    "\n",
    "def Image_search(Image_dir,num_image):\n",
    "    images=np.zeros([1,200*200*3])\n",
    "    for i in range(num_image):\n",
    "        image_raw = imread(Image_dir[i])\n",
    "        image = imresize(image_raw,(200,200))\n",
    "        image = np.reshape(image,(1,200*200*3))/255\n",
    "        image = image # - np.mean(image)\n",
    "        images = np.append(images,image,axis=0)\n",
    "    return images[1:]\n",
    "\n",
    "def label2onehot(label,num_class):\n",
    "    one_hot = np.zeros((len(label),num_class))\n",
    "    for idx,l in enumerate(label):\n",
    "        one_hot[idx,l] = 1\n",
    "    return one_hot\n",
    "\n",
    "search(\"/home/junhyun/Data/Dataset\")\n",
    "\n",
    "Total_images = np.zeros([1,200*200*3])\n",
    "Total_labels = np.zeros([1])\n",
    "\n",
    "Image_dir = search(\"/home/junhyun/Data/Dataset/Airplanes\")\n",
    "airplanes_images = Image_search(Image_dir,300)\n",
    "airplanes_labels = np.ones(len(airplanes_images))*0\n",
    "\n",
    "Total_images = np.append(Total_images,airplanes_images,axis=0)\n",
    "Total_labels = np.append(Total_labels,airplanes_labels,axis=0)\n",
    "\n",
    "\n",
    "Image_dir = search(\"/home/junhyun/Data/Dataset/Binocular\")\n",
    "binocular_images = Image_search(Image_dir,len(Image_dir))\n",
    "binocular_labels = np.ones(len(binocular_images))*1\n",
    "\n",
    "Total_images = np.append(Total_images,binocular_images,axis=0)\n",
    "Total_labels = np.append(Total_labels,binocular_labels,axis=0)\n",
    "\n",
    "Image_dir = search(\"/home/junhyun/Data/Dataset/grapes\")\n",
    "grapes_images = Image_search(Image_dir,len(Image_dir))\n",
    "grapes_labels = np.ones(len(grapes_images))*2\n",
    "\n",
    "Total_images = np.append(Total_images,grapes_images,axis=0)\n",
    "Total_labels = np.append(Total_labels,grapes_labels,axis=0)\n",
    "\n",
    "Image_dir = search(\"/home/junhyun/Data/Dataset/Leopards\")\n",
    "leopards_images = Image_search(Image_dir,len(Image_dir))\n",
    "leopards_labels = np.ones(len(leopards_images))*3\n",
    "\n",
    "Total_images = np.append(Total_images,leopards_images,axis=0)\n",
    "Total_labels = np.append(Total_labels,leopards_labels,axis=0)\n",
    "\n",
    "Image_dir = search(\"/home/junhyun/Data/Dataset/Motorbikes\")\n",
    "motorbikes_images = Image_search(Image_dir,300)\n",
    "motorbikes_labels = np.ones(len(motorbikes_images))*4\n",
    "\n",
    "Total_images = np.append(Total_images,motorbikes_images,axis=0)\n",
    "Total_labels = np.append(Total_labels,motorbikes_labels,axis=0)\n",
    "\n",
    "Image_dir = search(\"/home/junhyun/Data/Dataset/watch\")\n",
    "watch_images = Image_search(Image_dir,len(Image_dir))\n",
    "watch_labels = np.ones(len(watch_images))*5\n",
    "\n",
    "Total_images = np.append(Total_images,watch_images,axis=0)\n",
    "Total_labels = np.append(Total_labels,watch_labels,axis=0)\n",
    "\n",
    "Image_dir = search(\"/home/junhyun/Data/Dataset/Faces_easy\")\n",
    "faces_easy_images = Image_search(Image_dir,300)\n",
    "faces_easy_labels = np.ones(len(faces_easy_images))*6\n",
    "\n",
    "Total_images = np.append(Total_images,faces_easy_images,axis=0)[1:]\n",
    "Total_labels = np.append(Total_labels,faces_easy_labels,axis=0)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Total_one_hot = Total_labels.astype(int)\n",
    "Total_one_hot = label2onehot(np.transpose(Total_one_hot),7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_img_label(img,label):\n",
    "    tmp_img = np.zeros_like(img)\n",
    "    tmp_label = np.zeros_like(label)\n",
    "    Order = list(range(0,len(label)))\n",
    "    np.random.shuffle(Order)\n",
    "    for idx,order in enumerate(Order):\n",
    "        tmp_img[idx] = img[order]\n",
    "        tmp_label[idx] = label[order]\n",
    "    return tmp_img, tmp_label\n",
    "\n",
    "Shuffle_image, Shuffle_label = shuffle_img_label(Total_images,Total_one_hot)\n",
    "# Shuffle_image, Shuffle_label = shuffle_img_label(Total_images,Total_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RGB_flip_LeftRight(Img):\n",
    "    \n",
    "    Img_clone = copy(Img)\n",
    "    Flip_Img_result = np.zeros_like(Img_clone)\n",
    "\n",
    "    for i in range(Img_clone.shape[0]):\n",
    "        Flip_Img = np.reshape(Img_clone[i,:],[200,200,3])\n",
    "    \n",
    "        for k in range(3):\n",
    "            for j in range(Flip_Img.shape[0]):            \n",
    "                Flip_Img[j,:,k] = Flip_Img[j,::-1,k]\n",
    "    \n",
    "    Flip_Img = np.reshape(Flip_Img,[1,200*200*3])\n",
    "    Flip_Img_result[i,:] = Flip_Img\n",
    "\n",
    "    return Flip_Img_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_images = Shuffle_image[0:1500]\n",
    "Train_one_hot = Shuffle_label[0:1500]\n",
    "\n",
    "# Flip_images = RGB_flip_LeftRight(Train_images)\n",
    "\n",
    "# Train_images = np.append(Train_images,Flip_images,axis=0)\n",
    "# Train_one_hot = np.append(Train_one_hot,Train_one_hot,axis=0)\n",
    "\n",
    "\n",
    "Test_images = Shuffle_image[1500:]\n",
    "Test_one_hot = Shuffle_label[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=500\n",
    "batch_size=10\n",
    "learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess=sess\n",
    "        self.name=name\n",
    "        self._build_net()\n",
    "        \n",
    "    def _build_net(self):\n",
    "        self.training= tf.placeholder(tf.bool)\n",
    "        self.X = tf.placeholder(tf.float32,[None, 200*200*3])\n",
    "        X_img = tf.reshape(self.X, [-1,200,200,3])\n",
    "        self.Y = tf.placeholder(tf.float32,[None,7])\n",
    "        \n",
    "        \n",
    "#         self.W1 = tf.Variable(tf.random_normal([3, 3, 3, 64], stddev=0.01))\n",
    "        \n",
    "#         L1 = tf.nn.conv2d(X_img, self.W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#         L1 = tf.nn.relu(L1)\n",
    "        \n",
    "        self.conv_W = tf.layers.conv2d(inputs = X_img, \\\n",
    "                                       filters = 64, \\\n",
    "                                       kernel_size=[3, 3], \\\n",
    "                                       padding = \"SAME\",\\\n",
    "                                       activation=tf.nn.relu,\\\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        conv1 = tf.layers.conv2d(inputs = self.conv_W, filters = 64, kernel_size=[3, 3], padding = \"SAME\",\\\n",
    "                                 activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        conv12 = tf.layers.conv2d(inputs = conv1, filters = 64, kernel_size=[3, 3], padding = \"SAME\",\\\n",
    "                                  activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv12, pool_size = [2, 2], padding=\"SAME\", strides=2)\n",
    "#         drop1 = tf.layers.dropout(inputs=pool1, rate=0.7, training = self.training)\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(inputs = pool1, filters = 128, kernel_size=[3, 3], padding = \"SAME\", \\\n",
    "                                 activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        conv22 = tf.layers.conv2d(inputs = conv2, filters = 128, kernel_size=[3, 3], padding = \"SAME\", \\\n",
    "                                  activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        conv23 = tf.layers.conv2d(inputs = conv22, filters = 128, kernel_size=[3, 3], padding = \"SAME\", \\\n",
    "                                  activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv23, pool_size = [2, 2], padding=\"SAME\", strides=2)\n",
    "#         drop2 = tf.layers.dropout(inputs=pool2, rate=0.7, training = self.training)\n",
    "        \n",
    "        conv3 = tf.layers.conv2d(inputs = pool2, filters = 128, kernel_size=[3, 3], padding = \"SAME\", \\\n",
    "                                 activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        conv32 = tf.layers.conv2d(inputs = conv3, filters = 128, kernel_size=[3, 3], padding = \"SAME\", \\\n",
    "                                  activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        conv33 = tf.layers.conv2d(inputs = conv32, filters = 128, kernel_size=[3, 3], padding = \"SAME\", \\\n",
    "                                  activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        pool3 = tf.layers.max_pooling2d(inputs=conv33, pool_size = [2, 2], padding=\"SAME\", strides=2)\n",
    "#         drop3 = tf.layers.dropout(inputs=pool3, rate=0.7, training = self.training)\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(inputs = pool3, filters = 256, kernel_size=[3, 3], padding = \"SAME\", \\\n",
    "                                 activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        conv42 = tf.layers.conv2d(inputs = conv4, filters = 256, kernel_size=[3, 3], padding = \"SAME\", \\\n",
    "                                  activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        conv43 = tf.layers.conv2d(inputs = conv42, filters = 256, kernel_size=[3, 3], padding = \"SAME\", \\\n",
    "                                  activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        pool4 = tf.layers.max_pooling2d(inputs=conv43, pool_size = [2, 2], padding=\"SAME\", strides=2)\n",
    "#         drop4 = tf.layers.dropout(inputs=pool4, rate=0.7, training = self.training)\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(inputs = pool4, filters = 256, kernel_size=[3, 3], padding = \"SAME\", \\\n",
    "                                 activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        conv52 = tf.layers.conv2d(inputs = conv5, filters = 256, kernel_size=[3, 3], padding = \"SAME\", \\\n",
    "                                  activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        conv53 = tf.layers.conv2d(inputs = conv52, filters = 256, kernel_size=[3, 3], padding = \"SAME\", \\\n",
    "                                  activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer())        \n",
    "        pool5 = tf.layers.max_pooling2d(inputs=conv53, pool_size = [2, 2], padding=\"SAME\", strides=2)\n",
    "#         drop5 = tf.layers.dropout(inputs=pool5, rate=0.7, training = self.training)\n",
    "             \n",
    "#         conv6 = tf.layers.conv2d(inputs = pool5, filters = 512, kernel_size=[3, 3], padding = \"SAME\", activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "#         conv62 = tf.layers.conv2d(inputs = conv6, filters = 512, kernel_size=[3, 3], padding = \"SAME\", activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "#         conv63 = tf.layers.conv2d(inputs = conv62, filters = 512, kernel_size=[3, 3], padding = \"SAME\", activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))        \n",
    "#         pool6 = tf.layers.max_pooling2d(inputs=conv63, pool_size = [2, 2], padding=\"SAME\", strides=2)\n",
    "#         drop6 = tf.layers.dropout(inputs=pool6, rate=0.7, training = self.training)\n",
    "             \n",
    "        \n",
    "        flat = tf.reshape(pool5,[-1,7*7*256])\n",
    "        dense4 = tf.layers.dense(inputs=flat,units=4096,activation=tf.nn.relu,\\\n",
    "                                 kernel_initializer = tf.contrib.layers.xavier_initializer(),kernel_regularizer=tf.contrib.layers.l2_regularizer(0.001))\n",
    "        drop4 = tf.layers.dropout(inputs=dense4,rate=0.4,training = self.training)\n",
    "        batch4 = tf.layers.batch_normalization(drop4,training= True)\n",
    "        \n",
    "        dense5 = tf.layers.dense(inputs=batch4,units=4096,activation=tf.nn.relu,\\\n",
    "                                 kernel_initializer = tf.contrib.layers.xavier_initializer(),kernel_regularizer=tf.contrib.layers.l2_regularizer(0.001))\n",
    "        drop5 = tf.layers.dropout(inputs=dense5,rate=0.4,training = self.training)\n",
    "        batch5 = tf.layers.batch_normalization(drop5,training= True)\n",
    "\n",
    "\n",
    "        dense6 = tf.layers.dense(inputs=batch5,units=1000,activation=tf.nn.relu,\\\n",
    "                                 kernel_initializer = tf.contrib.layers.xavier_initializer(),kernel_regularizer=tf.contrib.layers.l2_regularizer(0.001))\n",
    "        drop6 = tf.layers.dropout(inputs=dense6,rate=0.4,training = self.training)\n",
    "        batch6 = tf.layers.batch_normalization(drop6,training= True)\n",
    "        \n",
    "        self.logits = tf.layers.dense(inputs=batch6,units=7,\\\n",
    "                                      kernel_initializer = tf.contrib.layers.xavier_initializer(),kernel_regularizer=tf.contrib.layers.l2_regularizer(0.001))\n",
    "        \n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits,labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits,1),tf.argmax(self.Y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        \n",
    "        \n",
    "    def predict(self,x_test,training=False):\n",
    "        return self.sess.run(self.logits,feed_dict={self.X:x_test, self.training:training})\n",
    "        \n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X:x_test, self.Y:y_test, self.training:training})\n",
    "        \n",
    "    def train(self,x_test,y_test,training=True):\n",
    "        return self.sess.run([self.cost,self.optimizer],feed_dict={self.X:x_test,self.Y:y_test, self.training:training})\n",
    "    \n",
    "    def get_weight(self):\n",
    "        Weight = sess.run(self.conv_W.kernel)\n",
    "        return Weight\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=gpu_config)\n",
    "\n",
    "models=[]\n",
    "num_models=1\n",
    "for m in range(num_models):\n",
    "    models.append(Model(sess,\"model\"+str(m)))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning start!!\n",
      "('epoch : ', '0001', ' cost : ', array([ 3.18382019]))\n",
      "('epoch : ', '0002', ' cost : ', array([ 3.110378]))\n",
      "('epoch : ', '0003', ' cost : ', array([ 2.6976722]))\n",
      "('epoch : ', '0004', ' cost : ', array([ 2.33882982]))\n",
      "('epoch : ', '0005', ' cost : ', array([ 2.11764714]))\n",
      "('epoch : ', '0006', ' cost : ', array([ 2.06678348]))\n",
      "('epoch : ', '0007', ' cost : ', array([ 2.03680726]))\n",
      "('epoch : ', '0008', ' cost : ', array([ 2.02287287]))\n",
      "('epoch : ', '0009', ' cost : ', array([ 1.98969529]))\n",
      "('epoch : ', '0010', ' cost : ', array([ 1.98689918]))\n",
      "('epoch : ', '0011', ' cost : ', array([ 1.98407033]))\n"
     ]
    }
   ],
   "source": [
    "print(\"Learning start!!\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    learning_rate = learning_rate#*0.99\n",
    "    avg_cost_list = np.zeros(len(models))\n",
    "    total_batch = int(len(Train_one_hot)/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs = Train_images[i*batch_size:(i+1)*batch_size]\n",
    "        batch_ys = Train_one_hot[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        for m_idx,m in enumerate(models):\n",
    "            c,_ = m.train(batch_xs,batch_ys)\n",
    "            avg_cost_list[m_idx] += c/total_batch\n",
    "    print(\"epoch : \", \"%04d\"%(epoch+1), \" cost : \", avg_cost_list)\n",
    "    \n",
    "print(\"Learning finished!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Accuracy = ', 0.16666669)\n",
      "(1, 'Accuracy = ', 0.20000002)\n",
      "('Ensemble Accuracy = ', 0.16666667)\n"
     ]
    }
   ],
   "source": [
    "Test_images = Shuffle_image[1530:1560]\n",
    "Test_one_hot = Shuffle_label[1530:1560]\n",
    "\n",
    "test_size = len(Test_one_hot)\n",
    "prediction = np.zeros(test_size*7).reshape(test_size,7)\n",
    "\n",
    "for m_idx,m in enumerate(models):\n",
    "    print(m_idx, \"Accuracy = \", m.get_accuracy(Test_images,Test_one_hot))\n",
    "    p = m.predict(Test_images)\n",
    "    prediction +=p\n",
    "\n",
    "ensemble_correct = tf.equal(tf.argmax(prediction,1),tf.argmax(Test_one_hot,1))\n",
    "ensemble_accuracy = tf.reduce_mean(tf.cast(ensemble_correct,tf.float32))\n",
    "\n",
    "print(\"Ensemble Accuracy = \", sess.run(ensemble_accuracy) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convWeight = sess.run(tf.transpose(models[0].get_weight()))\n",
    "Origin_data = Test_image[15][0:1024]\n",
    "Weight = convWeight[3][0]\n",
    "32*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Origin_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Origin_image = np.reshape(Origin_data,[32,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "\n",
    "i_width = 300\n",
    "i_height = 300\n",
    "\n",
    "Big_image = scipy.misc.imresize(Origin_image, (i_height, i_width))\n",
    "print(Big_image.shape)\n",
    "Origin_image = Big_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toimage(Origin_image).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Conv_image = signal.convolve2d(Origin_image,Weight,'same')\n",
    "\n",
    "Conv_image -= Conv_image.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toimage(Conv_image).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(Origin_image.max())\n",
    "print(Origin_image.min())\n",
    "\n",
    "print(Conv_image.max())\n",
    "print(Conv_image.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Edge_image = Origin_image-3*Conv_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(Edge_image.max())\n",
    "print(Edge_image.min())\n",
    "Edge_image -= Edge_image.min()\n",
    "Edge_image = Edge_image/Edge_image.max()\n",
    "print(Edge_image.max())\n",
    "print(Edge_image.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toimage(Edge_image).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toimage(Weight).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow kernel",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
